import os
import openai
import streamlit as st

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3, openai_api_key=api_key)

POLISHING_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ë°ì´í„°ë¥¼ í•´ì„í•˜ëŠ” ë°ì´í„° ê³¼í•™ìì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ë¶„ì„í•  í‘œì˜ rowëª… (index)ê³¼ columnëª…ì…ë‹ˆë‹¤.

row: {row_names}
column: {column_names}

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ”, ì‚¬ìš©ìì˜ ì§ˆë¬¸ ("{selected_question}")ê³¼ ê´€ë ¨í•´  
ë°ì´í„°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆì„ ë²•í•œ ê°€ì„¤(íŒ¨í„´, ê´€ê³„)ì„ 2~5ê°œ ì •ë„ ì œì•ˆí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì˜ˆì‹œ:
1. ì—°ë ¹ëŒ€ê°€ ë†’ì„ìˆ˜ë¡ ê´€ì‹¬ë„ê°€ ë†’ì„ ê²ƒì´ë‹¤.
2. ê¸°ì €ì§ˆí™˜ì´ ìˆëŠ” ê²½ìš° ê´€ì‹¬ë„ê°€ ë†’ì„ ê²ƒì´ë‹¤.

- ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í•©ë¦¬ì ì¸ ê°€ì„¤ë§Œ ìƒì„±í•  ê²ƒ
- ì™¸ë¶€ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
- ë¬¸ì¥ ê¸¸ì´ëŠ” ì§§ê³ , ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¡œ ì‘ì„±
"""

def streamlit_hypothesis_generate_fn(state):
    st.info("âœ… [Hypothesis Agent] Start hypothesis generation")
    selected_table = state["selected_table"]
    selected_question = state["selected_question"]

    # âœ… rowì™€ column name ì¶”ì¶œ
    if "ëŒ€ë¶„ë¥˜" in selected_table.columns and "ì†Œë¶„ë¥˜" in selected_table.columns:
        selected_table["row_name"] = selected_table["ëŒ€ë¶„ë¥˜"].astype(str) + "_" + selected_table["ì†Œë¶„ë¥˜"].astype(str)
        row_names = selected_table["row_name"].dropna().tolist()
    else:
        row_names = list(selected_table.index)

    column_names = list(selected_table.columns)

    row_names_str = ", ".join(map(str, row_names))
    column_names_str = ", ".join(map(str, column_names))

    # âœ… Streamlit - Table Overview ë¸”ë¡
    with st.container():
        st.markdown("### âœ… Table Overview")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ“ Row Names")
            preview_rows = ", ".join(row_names[:5]) + " ..." if len(row_names) > 5 else row_names_str
            st.markdown(f"**ë¯¸ë¦¬ë³´ê¸°:** {preview_rows}")
            with st.expander("ğŸ“‹ ì „ì²´ Row Names ë³´ê¸°"):
                st.markdown(row_names_str)

        with col2:
            st.markdown("#### ğŸ“ Column Names")
            preview_columns = ", ".join(column_names[:5]) + " ..." if len(column_names) > 5 else column_names_str
            st.markdown(f"**ë¯¸ë¦¬ë³´ê¸°:** {preview_columns}")
            with st.expander("ğŸ“‹ ì „ì²´ Column Names ë³´ê¸°"):
                st.markdown(column_names_str)

    # âœ… LLM í˜¸ì¶œ
    with st.spinner("ê°€ì„¤ ìƒì„± ì¤‘..."):
        response = llm.invoke(POLISHING_PROMPT.format(
            row_names=row_names_str,
            column_names=column_names_str,
            selected_question=selected_question
        ))

    hypotheses = response.content.strip()

    # âœ… Hypothesis ë¸”ë¡ â†’ ë°”ë¡œ ì „ì²´ ì¶œë ¥
    with st.container():
        st.markdown("### âœ… Generated Hypotheses")
        st.markdown(hypotheses)

    return {**state, "generated_hypotheses": hypotheses}

streamlit_hypothesis_generate_node = RunnableLambda(streamlit_hypothesis_generate_fn)