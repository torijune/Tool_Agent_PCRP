# variable_semantics_inference_agent.py
# [Step 1ï¸âƒ£] ë³€ìˆ˜ ì˜ë¯¸ ì¶”ë¡ : ì£¼ì–´ì§„ ë³€ìˆ˜ì™€ ê°’, ì½”ë”© ê°€ì´ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì¶”ë¡ í•˜ì„¸ìš”.  

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
llm = ChatOpenAI(temperature=0.2, model="gpt-4o")

# ğŸ¯ Step 1: ë³€ìˆ˜ ì˜ë¯¸ ì¶”ë¡  í”„ë¡¬í”„íŠ¸
VARIABLE_SEMANTICS_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ì¡°ì‚¬ raw dataë¥¼ ë¶„ì„í•˜ê³  ë³€ìˆ˜ ì˜ë¯¸ë¥¼ ì¶”ë¡ í•˜ëŠ” AI Assistantì…ë‹ˆë‹¤.

ì•„ë˜ ë°ì´í„°ë¥¼ ë³´ê³  ê° ë³€ìˆ˜ì˜ ì˜ë¯¸ë¥¼ ì‚¬ëŒì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•˜ì„¸ìš”.

[ëŒ€ë¶„ë¥˜ ê°’ ëª©ë¡]
{major_str}

[ì†Œë¶„ë¥˜ ê°’ ëª©ë¡]
{minor_str}

[Raw Data Code Guide (ì¼ë¶€)]
{code_guide_str}

[Raw Data ë³€ìˆ˜ ì„¤ëª…]
{raw_variables}

ì¶œë ¥ í˜•ì‹:
1. ë³€ìˆ˜ëª…: ì˜ë¯¸
2. ë³€ìˆ˜ëª…: ì˜ë¯¸
...
"""

def variable_semantics_inference_fn(state: dict) -> dict:
    major_str = state.get("major_str", "")
    minor_str = state.get("minor_str", "")
    code_guide_str = state.get("code_guide_str", "")
    raw_variables = state.get("raw_variables", "")

    prompt = VARIABLE_SEMANTICS_PROMPT.format(
        major_str=major_str,
        minor_str=minor_str,
        code_guide_str=code_guide_str,
        raw_variables=raw_variables
    )

    response = llm.invoke(prompt)
    semantics_result = response.content.strip()

    print("ğŸ” ë³€ìˆ˜ ì˜ë¯¸ ì¶”ë¡  ì™„ë£Œ.")
    return {**state, "semantics_result": semantics_result}

# âœ… LangGraph Node ì •ì˜
variable_semantics_node = RunnableLambda(variable_semantics_inference_fn)