# streamlit_table_analysis_agent.py

import streamlit as st
import pandas as pd
import os

from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

from dotenv import load_dotenv
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o", temperature=0.3)

TABLE_ANALYSIS_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸êµ¬ì§‘ë‹¨ ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê²½í–¥ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ì„¤ë¬¸ ì¡°ì‚¬ ì§ˆë¬¸:
{selected_question}

# í‘œ ë°ì´í„° (ì„ í˜•í™”ëœ í˜•íƒœ):
{linearized_table}

# ì‚¬ì „ì— ìƒì„±ëœ ê°€ì„¤ (ì°¸ê³ ìš©):
{generated_hypotheses}

# í†µê³„ ë¶„ì„ ê²°ê³¼ (F/T test ê¸°ë°˜ ìš”ì•½):
{ft_test_summary}

---

Let's think step by step.

ğŸ¯ ë¶„ì„ ë° ìš”ì•½ ì§€ì¹¨:

1. ë°˜ë“œì‹œ **F/T test ê²°ê³¼ì—ì„œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ëŒ€ë¶„ë¥˜ë§Œì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„**í•  ê²ƒ (p-value < 0.05, ìœ ì˜ì„± ë³„(*) ì¡´ì¬)
2. ëª¨ë“  ëŒ€ë¶„ë¥˜ / ì†Œë¶„ë¥˜ë¥¼ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ê²€ì • ê²°ê³¼ì—ì„œ ì°¨ì´ê°€ í¬ê³  ì˜ë¯¸ ìˆëŠ” ëŒ€ë¶„ë¥˜ë§Œ ì„ íƒì ìœ¼ë¡œ ì–¸ê¸‰**í•  ê²ƒ
3. **ì ˆëŒ€ í•´ì„í•˜ì§€ ë§ ê²ƒ**. ìˆ˜ì¹˜ì  ì°¨ì´ì— ëŒ€í•œ ì¸ê³¼ í•´ì„(ì˜ˆ: ê±´ê°•ì— ë¯¼ê°í•´ì„œ, ì£¼ë³€ì— ìˆì–´ì„œ ë“±)ì€ ëª¨ë‘ ê¸ˆì§€í•¨
4. ì™¸ë¶€ ë°°ê²½ì§€ì‹, ì£¼ê´€ì  ì¶”ë¡ , í•´ì„ì  ì–¸ê¸‰ì€ ì ˆëŒ€ ê¸ˆì§€. **í‘œë¡œë¶€í„° ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•œ ì‚¬ì‹¤ë§Œ ì„œìˆ **í•  ê²ƒ
5. ìˆ˜ì¹˜ ê¸°ë°˜ ê²½í–¥ì„ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì„œìˆ í•  ê²ƒ:
   - ìƒëŒ€ì ìœ¼ë¡œ ë” ë†’ì€ ê²½í–¥ ë³´ì˜€ìŒ
   - ë‚®ì€ ê°’ì„ ë‚˜íƒ€ëƒˆìŒ
6. ë³´ê³ ì„œ ìŒìŠ´ì²´ë¡œ ì‘ì„±í•  ê²ƒ (ì˜ˆ: ~í–ˆìŒ, ~ë¡œ ë‚˜íƒ€ë‚¬ìŒ)
7. ë¬¸ì¥ ê°„ ì—°ê²°ì–´ë¥¼ í™œìš©í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ í•˜ê³ , ë„ˆë¬´ ë‹¨ì¡°ë¡­ê±°ë‚˜ ë°˜ë³µì ì¸ í‘œí˜„ (~í–ˆìŒ. ~í–ˆìŒ.)ì€ í”¼í•  ê²ƒ
8. **ìœ ì˜ì„±ì´ ì—†ê±°ë‚˜, ê²€ì •ì—ì„œ ì œì™¸ëœ í•­ëª©ì€ ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ**
9. **íŠ¹ì • ëŒ€ë¶„ë¥˜ê°€ ê°€ì¥ ë‘ë“œëŸ¬ì§„ ì°¨ì´ë¥¼ ë³´ì˜€ì„ ê²½ìš°**, í•´ë‹¹ ê²½í–¥ì„ ê°•ì¡°í•  ê²ƒ
10. ìˆ«ìê°’ì„ ì§ì ‘ ì“°ì§€ ë§ê³  ìƒëŒ€ì ì¸ ê²½í–¥ë§Œ ì–¸ê¸‰í•  ê²ƒ

---

ğŸ“ ì¶œë ¥ ì˜ˆì‹œ:

ê¸°ì €ì§ˆí™˜ ìˆëŠ” ê·¸ë£¹, ëŒ€ê¸°ì˜¤ì—¼ ë°°ì¶œì‚¬ì—…ì¥ ì£¼ë³€ ê±°ì£¼ ê·¸ë£¹ì—ì„œ ëŒ€ê¸°í™˜ê²½ ê´€ì‹¬ ì •ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì•˜ìŒ. ì—°ë ¹ëŒ€ê°€ ë†’ì„ìˆ˜ë¡ ê´€ì‹¬ë„ê°€ ì¦ê°€í•˜ëŠ” ê²½í–¥ë„ ê´€ì°°ë¨.
"""

def streamlit_table_anaylsis_node_fn(state):
    st.info("âœ… [Table Analysis Agent] Start table analyzing")

    linearized_table = state["linearized_table"]
    ft_test_summary = state["ft_test_summary"]
    selected_question = state["selected_question"]
    generated_hypotheses = state["generated_hypotheses"]

    with st.spinner("LLM ë¶„ì„ ì¤‘..."):
        prompt = TABLE_ANALYSIS_PROMPT.format(
            selected_question=selected_question,
            linearized_table=linearized_table,
            ft_test_summary=str(ft_test_summary),
            generated_hypotheses = generated_hypotheses
        )

        response = llm.invoke(prompt)
        table_analysis = response.content.strip()

    # âœ… Custom ìŠ¤íƒ€ì¼ ì ìš©
    st.markdown("""
        <style>
        .big-text {
            font-size: 17px !important;
            line-height: 1.7;
            max-width: 1200px;
        }
        </style>
        """, unsafe_allow_html=True)

    # âœ… Table Analysis ì¶œë ¥
    st.markdown("### ğŸ“‹ Table Analysis ìš”ì•½ ê²°ê³¼")
    st.markdown(f"<div class='big-text'>{table_analysis}</div>", unsafe_allow_html=True)

    return {**state, "table_analysis": table_analysis}

streamlit_table_anaylsis_node = RunnableLambda(streamlit_table_anaylsis_node_fn)