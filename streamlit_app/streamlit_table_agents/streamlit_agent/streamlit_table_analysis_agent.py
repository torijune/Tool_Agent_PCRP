import os
import openai

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

import streamlit as st

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

TABLE_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸êµ¬ì§‘ë‹¨ ê°„ íŒ¨í„´ê³¼ ê²½í–¥ì„±ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ“ ì„¤ë¬¸ ì¡°ì‚¬ ì§ˆë¬¸:
{selected_question}

ğŸ“Š í‘œ ë°ì´í„° (ì„ í˜•í™”ëœ í˜•íƒœ):
{linearized_table}

ğŸ“ˆ ìˆ˜ì¹˜ ë¶„ì„ ê²°ê³¼ (ëŒ€ë¶„ë¥˜ë³„ í•­ëª©ë³„ ìµœê³ /ìµœì € ê°’, í‘œì¤€í¸ì°¨, ë²”ìœ„ ë“±):
{numeric_anaylsis}

ğŸ’¡ ë°ì´í„° ê¸°ë°˜ ìë™ ìƒì„± ê°€ì„¤ ëª©ë¡ (ì°¸ê³ ìš©):
{generated_hypotheses}

---

Let's think step by step

ğŸ¯ ë¶„ì„ ë° ìš”ì•½ ì§€ì¹¨:

1. ì£¼ìš” ëŒ€ë¶„ë¥˜ (ì˜ˆ: ì—°ë ¹ëŒ€, ê¸°ì €ì§ˆí™˜ ì—¬ë¶€, ëŒ€ê¸°ì˜¤ì—¼ ë°°ì¶œì‚¬ì—…ì¥ ìœ ë¬´, ì£¼ìš” ì²´ë¥˜ ê³µê°„ ë“±)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆëŠ”ì§€ ë¶„ì„í•  ê²ƒ
2. ì˜ë¯¸ ìˆëŠ” ì°¨ì´ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ìš” ê·¸ë£¹ë§Œ ì„ íƒì ìœ¼ë¡œ ì–¸ê¸‰í•  ê²ƒ (ì˜ˆ: í‰ê·  ê°’, % ë¹„ìœ¨ ì°¨ì´ê°€ í¬ê±°ë‚˜ ë‘ë“œëŸ¬ì§„ ê²½ìš°ë§Œ)
3. ì„¸ë¶€ ì†Œë¶„ë¥˜ (ì˜ˆ: ì„±ë³„, 20ëŒ€/30ëŒ€ ë“±)ëŠ” ì˜ë¯¸ ì°¨ì´ê°€ ëª…í™•í•˜ê²Œ í´ ê²½ìš°ì—ë§Œ ì„ íƒì ìœ¼ë¡œ ì–¸ê¸‰í•  ê²ƒ
4. ëª¨ë“  ì„¸ë¶€ ê·¸ë£¹ ë˜ëŠ” rowë¥¼ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ (ë¶ˆí•„ìš”í•œ row ì„¤ëª… ê¸ˆì§€)
5. ì™¸ë¶€ ë°°ê²½ì§€ì‹, ì£¼ê´€ì  í•´ì„ ì—†ì´ ì˜¤ì§ í‘œì™€ ìˆ˜ì¹˜ ê¸°ë°˜ì˜ ì‚¬ì‹¤ë§Œ ì‘ì„±í•  ê²ƒ
6. ìˆ«ì ê¸°ë°˜ ê²½í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ "ìƒëŒ€ì ìœ¼ë¡œ ë” ë†’ì€ ê²½í–¥ ë³´ì˜€ìŒ", "ëšœë ·í•œ ì°¨ì´ë¥¼ ë‚˜íƒ€ëƒˆìŒ", "ë‘ë“œëŸ¬ì§„ ê²½í–¥ ë³´ì˜€ìŒ" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
7. ì§€ë‚˜ì¹˜ê²Œ ë‹¨ì ˆì  (~í–ˆìŒ. ~í–ˆìŒ. ë°˜ë³µ) í‘œí˜„ì„ ì§€ì–‘í•˜ê³ , ê´€ë ¨ëœ ê·¸ë£¹ë“¤ì€ ì—°ê²°ì–´ë¥¼ ì‚¬ìš©í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ì„ ê²ƒ
8. ë™ì¼ ì˜ë¯¸ì˜ ê·¸ë£¹ì€ ì¤‘ë³µ í‘œí˜„ì„ í”¼í•˜ê³  ë¬¸ì¥ ê°€ë…ì„±ì„ ë†’ì¼ ê²ƒ
9. ì •í™•í•œ ìˆ˜ì¹˜ê°’(ì˜ˆ: 45.3%)ì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ìˆ˜ì¹˜ ì°¨ì´ì— ê¸°ë°˜í•œ ìƒëŒ€ì  ê²½í–¥ë§Œ ì„œìˆ í•  ê²ƒ
10. ë°˜ë“œì‹œ 'ë°ì´í„° ê¸°ë°˜ ìë™ ìƒì„± ê°€ì„¤ ëª©ë¡'ì„ ì°¸ê³ í•˜ì—¬ ê°€ì„¤ì„ ê²€ì¦í•˜ê±°ë‚˜ í•´ë‹¹ ê°€ì„¤ê³¼ ê´€ë ¨ëœ íŒ¨í„´ì„ íƒìƒ‰í•˜ê³  ë¶„ì„ ê²°ê³¼ì— ë°˜ì˜í•  ê²ƒ

---

ğŸ“ ì¶œë ¥ í˜•ì‹:

- ì œëª©, ë¶ˆë¦¿, ë¦¬ìŠ¤íŠ¸ ì—†ì´ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
- ì§§ê³  ëª…í™•í•˜ê²Œ ë¶„ì„ ê²°ê³¼ë§Œ ìš”ì•½í•  ê²ƒ
- ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ì‘ì„±í•  ê²ƒ

ì˜ˆì‹œ:
ëŒ€ê¸°í™˜ê²½ ë¬¸ì œ ê´€ì‹¬ ì •ë„, ì—°ë ¹ëŒ€ ë†’ì„ìˆ˜ë¡ ë‘ë“œëŸ¬ì§„ ê²½í–¥ ë³´ì˜€ìŒ. ê¸°ì €ì§ˆí™˜ ìˆëŠ” ê·¸ë£¹, ëŒ€ê¸°ì˜¤ì—¼ ë°°ì¶œì‚¬ì—…ì¥ ì£¼ë³€ ê±°ì£¼ ê·¸ë£¹, ì‹¤ì™¸ ì²´ë¥˜ì‹œê°„ ë§ì€ ê·¸ë£¹ë„ ìƒëŒ€ì ìœ¼ë¡œ ë” ë†’ì€ ê´€ì‹¬ ë³´ì˜€ìŒ.
"""

def streamlit_table_anaylsis_node_fn(state):
    st.info("âœ… [Table Analysis Agent] Start table analysis")

    linearized_table = state["linearized_table"]
    numeric_anaylsis = state["numeric_anaylsis"]
    selected_question = state["selected_question"]
    generated_hypotheses = state["generated_hypotheses"]

    prompt = TABLE_PROMPT.format(
        selected_question=selected_question,
        linearized_table=linearized_table,
        numeric_anaylsis=numeric_anaylsis,
        generated_hypotheses=generated_hypotheses
    )

    with st.spinner("í…Œì´ë¸” ë¶„ì„ LLM ì§„í–‰ ì¤‘..."):
        response = llm.invoke(prompt)

    table_analysis = response.content.strip()

    st.markdown("### âœ… Draft Generated Report")
    st.markdown(table_analysis)

    return {**state, "table_analysis": table_analysis}

streamlit_table_anaylsis_node = RunnableLambda(streamlit_table_anaylsis_node_fn)