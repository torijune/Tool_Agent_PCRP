import streamlit as st
import io
import os
import traceback
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    from dotenv import load_dotenv
    # Try importing your custom modules with error handling
    try:
        from streamlit_table_workflow_graph import build_table_graph
        from streamlit_table_parser import load_survey_tables
    except ImportError as e:
        st.error(f"âŒ Failed to import required modules: {e}")
        logger.error(f"Import error: {e}")
        st.stop()
except ImportError as e:
    st.error(f"âŒ Failed to import dotenv: {e}")
    logger.error(f"Dotenv import error: {e}")
    st.stop()

# ğŸ”‘ í™˜ê²½ ë³€ìˆ˜ ë¡œë”© (.env ë¡œì»¬ + st.secrets ë°°í¬ìš©)
try:
    load_dotenv()
    # Log API key status (without revealing the key)
    if "OPENAI_API_KEY" in os.environ:
        logger.info("OPENAI_API_KEY found in environment variables")
    
    if "OPENAI_API_KEY" in st.secrets:
        os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
        logger.info("OPENAI_API_KEY loaded from st.secrets")
    else:
        logger.warning("OPENAI_API_KEY not found in st.secrets")
        
    # Verify API key is set
    if "OPENAI_API_KEY" not in os.environ:
        st.warning("âš ï¸ OPENAI_API_KEY not set. Some features may not work.")
        logger.warning("OPENAI_API_KEY not set in environment")
except Exception as e:
    st.error(f"âŒ Error loading environment variables: {e}")
    logger.error(f"Environment error: {e}")

def normalize_key(key: str) -> str:
    return key.replace("-", "_").lower()

def main():
    try:
        st.set_page_config(page_title="Table Analysis Agent", layout="wide")
        st.title("ğŸ“Š Table Analysis Multi-Agent Demo")

        st.markdown("""
        - Excel í†µê³„í‘œ ê¸°ë°˜ ë¶„ì„ ìë™í™”
        - Upload File â†’ Parsing â†’ Hypothesis â†’ Numeric Analysis â†’ Table Analysis â†’ Hallucination Check â†’ Revision â†’ Polishing
        """)

        # âœ… ì‚¬ì´ë“œë°”: ì—…ë¡œë“œ
        with st.sidebar:
            st.header("1ï¸âƒ£ ë¶„ì„ìš© Excel íŒŒì¼ ì—…ë¡œë“œ (í†µê³„í‘œ)")
            uploaded_file = st.file_uploader("ğŸ“¥ ë¶„ì„ìš© Excel íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=None)

            st.header("2ï¸âƒ£ ì›ì‹œ ë°ì´í„° Excel íŒŒì¼ ì—…ë¡œë“œ (Raw DATA, ë³€ìˆ˜, ì½”ë”©ê°€ì´ë“œ, ë¬¸í•­ í¬í•¨)")
            raw_data_file = st.file_uploader("ğŸ“¥ ì›ì‹œ ë°ì´í„° Excel íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=None, key="raw_data")

            st.header("3ï¸âƒ£ ë¶„ì„ ë°©ì‹ ì„ íƒ")
            analysis_type = st.radio(
                "ë¶„ì„ ë°©ì‹",
                ["ë‹¨ì¼ ì§ˆë¬¸ ì„ íƒ - ì§ì ‘ ì„ íƒ", "ì „ì²´ ì§ˆë¬¸ batch - ì „ì²´ ìë™ ë¶„ì„"],
                index=0
            )
            analysis_type_flag = analysis_type.startswith("ë‹¨ì¼")

        selected_question_key = None
        selected_table = None
        tables = None
        question_texts = None
        question_keys = None

        if uploaded_file:
            try:
                logger.info("Loading tables from uploaded file")
                file_bytes = io.BytesIO(uploaded_file.read())
                # Reset file pointer for future use
                uploaded_file.seek(0)
                
                tables, question_texts, question_keys = load_survey_tables(file_bytes)
                logger.info(f"Successfully loaded {len(tables)} tables")
            except Exception as e:
                logger.error(f"Error loading tables: {traceback.format_exc()}")
                st.error(f"âŒ ì—…ë¡œë“œëœ í†µê³„í‘œ íŒŒì¼ì—ì„œ í…Œì´ë¸”ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.stop()

            if analysis_type_flag and tables is not None:
                st.subheader("4ï¸âƒ£ ë¶„ì„í•  ì§ˆë¬¸ ì„ íƒ")
                st.info("ğŸ“‹ ì—…ë¡œë“œí•œ ë¶„ì„ìš© Excel íŒŒì¼ì˜ ì§ˆë¬¸ ëª©ë¡ì…ë‹ˆë‹¤. ë¶„ì„í•  ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.")

                normalized_question_texts = {normalize_key(k): v for k, v in question_texts.items()}
                options = []
                for key in question_keys:
                    norm_key = normalize_key(key)
                    if norm_key in normalized_question_texts:
                        label = normalized_question_texts[norm_key]
                        options.append(f"[{key}] {label}")
                    else:
                        st.warning(f"âš ï¸ ì§ˆë¬¸ í…ìŠ¤íŠ¸ ëˆ„ë½: '{key}'")
                        logger.warning(f"Missing question text for key: {key}")

                if not options:
                    st.error("âŒ ìœ íš¨í•œ ì§ˆë¬¸ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    logger.error("No valid question texts found")
                    st.stop()

                selected_option = st.selectbox("ì§ˆë¬¸ ëª©ë¡", options)
                selected_index = options.index(selected_option)
                selected_question_key = question_keys[selected_index]
                selected_table = tables[selected_question_key]

                st.success(f"âœ… ì„ íƒëœ ì§ˆë¬¸: {question_texts.get(selected_question_key, selected_question_key)}")
                st.dataframe(selected_table.head(), use_container_width=True)
            elif not analysis_type_flag:
                st.info("ğŸ“Œ ì „ì²´ batch ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° ì§ˆë¬¸ì€ ìë™ ë¶„ì„ë©ë‹ˆë‹¤.")

        run = st.button("ğŸš€ ë¶„ì„ ì‹œì‘", use_container_width=True)

        if run:
            if uploaded_file is None:
                st.error("â— ë¶„ì„ìš© Excel íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
                logger.error("Analysis started without uploaded file")
                st.stop()
            if raw_data_file is None:
                st.error("â— ì›ì‹œ ë°ì´í„° Excel íŒŒì¼ë„ ë°˜ë“œì‹œ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
                logger.error("Analysis started without raw data file")
                st.stop()

            try:
                logger.info("Reading raw data file")
                raw_data_stream = io.BytesIO(raw_data_file.read())
                # Reset file pointer for future use
                raw_data_file.seek(0)
            except Exception as e:
                logger.error(f"Raw data file processing error: {traceback.format_exc()}")
                st.error(f"âŒ ì›ì‹œ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                st.stop()

            try:
                logger.info("Building table graph workflow")
                workflow = build_table_graph()
            except Exception as e:
                logger.error(f"Error building workflow: {traceback.format_exc()}")
                st.error(f"âŒ ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.stop()

            init_state = {
                "analysis_type": analysis_type_flag,
                "uploaded_file": io.BytesIO(uploaded_file.read()),
                "raw_data_file": raw_data_stream,
            }

            if analysis_type_flag and selected_question_key is not None:
                init_state["selected_key"] = selected_question_key

            try:
                logger.info("Invoking workflow")
                with st.spinner("ë¶„ì„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
                    result = workflow.invoke(init_state)
                logger.info("Workflow completed successfully")
            except Exception as e:
                logger.error(f"Workflow execution error: {traceback.format_exc()}")
                st.error(f"âŒ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                st.stop()

            st.success("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")

            if "polishing_result" in result:
                st.markdown("### ğŸ” ìµœì¢… ìš”ì•½ ê²°ê³¼")
                st.text_area("Polished Report", result["polishing_result"], height=300)
            else:
                st.warning("âš ï¸ ìµœì¢… ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logger.warning("No polishing_result in workflow output")

            st.markdown("### ğŸ“ ì „ì²´ ìƒíƒœ ë³´ê¸°")
            st.json(result)
    except Exception as e:
        logger.error(f"Unhandled exception: {traceback.format_exc()}")
        st.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    main()