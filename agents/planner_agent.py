import os
import openai
import json
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda
from agents.tools_schema import tools_schema

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ”§ ìµœì‹  ë„êµ¬ ì„¤ëª…ì´ í¬í•¨ëœ function_call ê¸°ë°˜ ëª¨ë¸ ìƒì„±
llm = ChatOpenAI(
    model="gpt-4o",  # function calling ì§€ì› ëª¨ë¸
    temperature=0.2,
    model_kwargs={"functions": tools_schema}
)

# âœ… í”„ë¡¬í”„íŠ¸ ìˆ˜ì •: ë„êµ¬ ì´ë¦„ì„ ì§ì ‘ ì“°ëŠ” ëŒ€ì‹  í•¨ìˆ˜ ì„¤ëª… ê¸°ë°˜ìœ¼ë¡œ ì„ íƒ ìœ ë„
PLANNER_PROMPT = """
You are a smart tool-planning assistant. Based on the user query, select the best tool and explain your choice in the `reason` field.

Use the function schema provided to you.

Return only a function_call to `use_tool` with:
- tool_name: the selected tool (as defined in the schema)
- reason: short justification
"""

def planner_fn(state: dict) -> dict:
    query = state["query"]
    messages = [
        {"role": "system", "content": PLANNER_PROMPT.strip()},
        {"role": "user", "content": query}
    ]

    # â¬ í•¨ìˆ˜ í˜¸ì¶œ ìœ ë„
    response = llm.invoke(messages)
    function_call = response.additional_kwargs.get("function_call", {})

    try:
        arguments_raw = function_call.get("arguments", "{}")
        arguments = json.loads(arguments_raw)

        tool_name = arguments.get("tool_name", "")
        reason = arguments.get("reason", "")

        print(f"ğŸ§­ ì„ íƒëœ ë„êµ¬: {tool_name} â†’ {reason}")
        return {**state, "plan": tool_name, "plan_desc": reason}
    except Exception as e:
        print(f"âŒ Function call ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {**state, "plan": "", "plan_desc": ""}
        
planner_node = RunnableLambda(planner_fn)