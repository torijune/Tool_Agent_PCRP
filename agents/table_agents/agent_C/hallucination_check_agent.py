import os
import openai

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)


HALLUCINATION_CHECK_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ í•´ì„ ê²°ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ì˜ í…Œì´ë¸” ë°ì´í„°ì™€ ìˆ˜ì¹˜ ë¶„ì„ ê²°ê³¼, ê·¸ë¦¬ê³  í•´ë‹¹ í…Œì´ë¸”ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ìš”ì•½ ê²°ê³¼ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.

ğŸ“ ì„¤ë¬¸ ë¬¸í•­:
{selected_question}

ğŸ“Š ì„ í˜•í™”ëœ í…Œì´ë¸”:
{linearized_table}

ğŸ“ˆ ìˆ˜ì¹˜ ë¶„ì„ ê²°ê³¼:
{numeric_anaylsis}

ğŸ§¾ ìƒì„±ëœ ìš”ì•½:
{table_analysis}

ì´ ìš”ì•½ì´ ìœ„ì˜ í‘œì™€ ìˆ˜ì¹˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

- í‘œì—ì„œ ìœ ì¶”í•  ìˆ˜ ì—†ëŠ” ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì´ ìˆë‹¤ë©´ "reject: [ì´ìœ ]" í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- ë§Œì•½ ìš”ì•½ì´ ì¶©ë¶„íˆ ê°ê´€ì ì´ê³  ìˆ˜ì¹˜ ê¸°ë°˜ì´ë©´ "accept"ë¼ê³ ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

def hallucination_check_node_fn(state):
    print("*" * 10, "Start table analysis hallucination check", "*" * 10)
    prompt = HALLUCINATION_CHECK_PROMPT.format(
        selected_question=state["selected_question"],
        linearized_table=state["linearized_table"],
        numeric_anaylsis=state["numeric_anaylsis"],
        table_analysis=state["table_analysis"]
    )

    response = llm.invoke(prompt)
    result = response.content.strip()

    # "reject: ì´ìœ " í˜¹ì€ "accept"
    if result.lower().startswith("reject"):
        decision = "reject"
        feedback = result[len("reject"):].strip(": ").strip()
        print("Hallucination Check ê²°ê³¼: ", decision)
        print("\nLLM Feedback: ", feedback)
    else:
        decision = "accept"
        print("Hallucination Check ê²°ê³¼: ", decision)
        feedback = ""

    return {**state, "hallucination_check": decision, "feedback": feedback}

hallucination_check_node = RunnableLambda(hallucination_check_node_fn)