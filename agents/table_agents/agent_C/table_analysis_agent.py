import os
import openai

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

TABLE_PROMPT = """
ë‹¹ì‹ ì€ í†µê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸êµ¬ì§‘ë‹¨ ê°„ íŒ¨í„´ê³¼ ê²½í–¥ì„±ì„ ê°ê´€ì ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ“ ì„¤ë¬¸ ì¡°ì‚¬ ì§ˆë¬¸:
{selected_question}

ğŸ“Š í‘œ ë°ì´í„° (ì„ í˜•í™”ëœ í˜•íƒœ):
{linearized_table}

ğŸ“ˆ ìˆ˜ì¹˜ ë¶„ì„ ê²°ê³¼ (ëŒ€ë¶„ë¥˜ë³„ í•­ëª©ë³„ ìµœê³ /ìµœì € ê°’, í‘œì¤€í¸ì°¨, ë²”ìœ„ ë“±):
{numeric_anaylsis}

ğŸ’¡ ë°ì´í„° ê¸°ë°˜ ìë™ ìƒì„± ê°€ì„¤ ëª©ë¡ (ì°¸ê³ ìš©):
{generated_hypotheses}

---

Let's think step by step

ğŸ¯ ë¶„ì„ ë° ìš”ì•½ ì§€ì¹¨:

1. ì£¼ìš” ëŒ€ë¶„ë¥˜ (ì˜ˆ: ì—°ë ¹ëŒ€, ê¸°ì €ì§ˆí™˜ ì—¬ë¶€, ëŒ€ê¸°ì˜¤ì—¼ ë°°ì¶œì‚¬ì—…ì¥ ìœ ë¬´, ì£¼ìš” ì²´ë¥˜ ê³µê°„ ë“±)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•  ê²ƒ
2. ì„¸ë¶€ ì†Œë¶„ë¥˜ (ì˜ˆ: ì„±ë³„, 20ëŒ€/30ëŒ€ ë“± ì„¸ë¶€ ì—°ë ¹ êµ¬ê°„)ëŠ” ëª…í™•í•œ ì°¨ì´ê°€ ì—†ì„ ê²½ìš° ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ ê²ƒ
3. ì˜ë¯¸ ìˆëŠ” ì°¨ì´ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ìš” ê·¸ë£¹ë§Œ ì„ íƒì ìœ¼ë¡œ ì–¸ê¸‰í•  ê²ƒ
4. ëª¨ë“  ì„¸ë¶€ ê·¸ë£¹ì„ ë‚˜ì—´í•˜ì§€ ë§ê³ , íŠ¹ì§•ì ì¸ ê·¸ë£¹ê³¼ ì£¼ìš” ì°¨ì´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•  ê²ƒ
5. ì™¸ë¶€ ë°°ê²½ì§€ì‹, ì£¼ê´€ì  í•´ì„ ì—†ì´ ì˜¤ì§ ìˆ˜ì¹˜ ê¸°ë°˜ì˜ ì‚¬ì‹¤ë§Œ ì‘ì„±í•  ê²ƒ
6. ìˆ«ì ê¸°ë°˜ì˜ ê²½í–¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ "ìƒëŒ€ì ìœ¼ë¡œ ë” ë†’ì€ ê²½í–¥ ë³´ì˜€ìŒ", "ë‚®ì€ ê°’ì„ ë‚˜íƒ€ëƒˆìŒ" ë“± **ìŒìŠ´ì²´**ë¡œ ì‘ì„±í•  ê²ƒ
7. ë¬¸ì¥ì€ í‰ì„œë¬¸ì´ ì•„ë‹Œ, **ë³´ê³ ì„œ ìŒìŠ´ì²´ ìŠ¤íƒ€ì¼**ë¡œ ì‘ì„±í•  ê²ƒ (ì˜ˆ: ~í–ˆìŒ, ~ë¡œ ë‚˜íƒ€ë‚¬ìŒ)
8. ì§€ë‚˜ì¹˜ê²Œ ë‹¨ì ˆì  (~í–ˆìŒ. ~í–ˆìŒ. ë°˜ë³µ) í‘œí˜„ì„ ì§€ì–‘í•˜ê³ , ê´€ë ¨ëœ ê·¸ë£¹ë“¤ì€ **ì—°ê²°ì–´ë¥¼ í™œìš©í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ì„ ê²ƒ**
9. ê°€ë…ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë™ì¼í•œ ì˜ë¯¸ì˜ ê·¸ë£¹ì´ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•  ê²ƒ
10. ë³´ê³ ì„œì— ì •í™•í•œ ìˆ˜ì¹˜ëŠ” ì“°ì§€ ë§ê³ , ìˆ˜ì¹˜ ì°¨ì´ì— ê¸°ë°˜í•œ ê²½í–¥ë§Œ ì„œìˆ í•  ê²ƒ
11. ìœ„ì˜ 'ë°ì´í„° ê¸°ë°˜ ê°€ì„¤'ì„ ì°¸ê³ í•´ í•´ë‹¹ ê°€ì„¤ì„ ê²€ì¦í•˜ê±°ë‚˜ ê´€ë ¨ ê²½í–¥ì„±ì„ ë°œê²¬í•˜ë ¤ ë…¸ë ¥í•  ê²ƒ

---

ğŸ“ ì¶œë ¥ í˜•ì‹:

- ì œëª©, ë¶ˆë¦¿, ë¦¬ìŠ¤íŠ¸ ì—†ì´ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
- ì§§ê³  ëª…í™•í•˜ê²Œ ë¶„ì„ ê²°ê³¼ë§Œ ìš”ì•½í•  ê²ƒ
- ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ì‘ì„±í•  ê²ƒ

ì˜ˆì‹œ:
ëŒ€ê¸°í™˜ê²½ ë¬¸ì œ ê´€ì‹¬ ì •ë„, ì—°ë ¹ëŒ€ ë†’ì„ìˆ˜ë¡ ë” ë†’ì€ ê²½í–¥ ë³´ì˜€ìŒ. ê¸°ì €ì§ˆí™˜ ìˆëŠ” ê·¸ë£¹, ëŒ€ê¸°ì˜¤ì—¼ ë°°ì¶œì‚¬ì—…ì¥ ì£¼ë³€ ê±°ì£¼ ê·¸ë£¹, ì‹¤ì™¸ ì²´ë¥˜ì‹œê°„ ë§ì€ ê·¸ë£¹ë„ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ê´€ì‹¬ ë³´ì˜€ìŒ.
"""

def table_anaylsis_node_fn(state):
    print("*" * 10, "Start table anaylzing", "*" * 10)
    linearized_table = state["linearized_table"]
    numeric_anaylsis = state["numeric_anaylsis"]
    selected_question = state["selected_question"]
    generated_hypotheses = state["generated_hypotheses"]

    response = llm.invoke(TABLE_PROMPT.format(selected_question = selected_question,
                                               linearized_table=linearized_table,
                                                 numeric_anaylsis=numeric_anaylsis,
                                                 generated_hypotheses = generated_hypotheses))
    table_analysis = response.content.strip()

    # print("ğŸ’¬ Table Anaylsis ì‹œì‘")
    # print(f"ğŸ“ Linearlized Table:\n{linearized_table}")
    # print(f"ğŸ“„ ê²€ìƒ‰ëœ ë…¼ë¬¸ ìš”ì•½ (ë¯¸ë¦¬ë³´ê¸°):\n{retrieved_doc[:300]}...")
    print("ìƒì„±ëœ ë³´ê³ ì„œ ì´ˆì•ˆ :", table_analysis)
    return {**state, "table_analysis": table_analysis}

table_anaylsis_node = RunnableLambda(table_anaylsis_node_fn)