import os
import openai
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… Critic LLM ì„¤ì •: ì ë‹¹í•œ ë‹¤ì–‘ì„±ê³¼ íŒë‹¨ í—ˆìš©ì„ ìœ„í•´ temperature = 0.5
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

# âœ… íŒë‹¨ ê¸°ì¤€ì„ ê°•í™”í•œ Critic Prompt
CRITIC_PROMPT = """
You are a critical evaluator of an LLM-based agent system.
Your task is to decide whether the tool's result sufficiently answers the user query, in accordance with the plan.

Respond with:
- "accept" if the tool result is correct, relevant, and matches the user's intent.
- "reject" if the result is incorrect, incomplete, or unrelated.

Only return one word: accept or reject.

Query: {query}
Plan: {plan}
Tool Result: {tool_result}
"""

def critic_fn(state: dict) -> dict:
    query = state.get("query", "")
    plan = state.get("plan", "")
    result = state.get("tool_result", "")
    
    # âœ… ì „ì²´ ì…ë ¥ ë¡œê·¸ í™•ì¸ìš©
    # print("\n--- Critic Input ---")
    # print(f"Query: {query}")
    # print(f"Plan: {plan}")
    # print(f"Tool Result: {result}")
    
    response = llm.invoke(CRITIC_PROMPT.format(query=query, plan=plan, tool_result=result))
    decision = response.content.strip().lower()

    # print("ğŸ” Critic Decision:", decision)

    # âœ… fail-safe: í—ˆìš©ëœ ê°’ ì™¸ì˜ ì¶œë ¥ì€ ê°•ì œ reject
    if decision not in ["accept", "reject"]:
        decision = "reject"

    return {**state, "decision": decision}

critic_node = RunnableLambda(critic_fn)