# Target URL: 
# https://aclanthology.org/volumes/2024.emnlp-main/
# https://aclanthology.org/volumes/2024.acl-long/
# https://aclanthology.org/volumes/2024.naacl-long/
# title class= "align-middle", í•´ë‹¹ titleì˜ ë§í¬ : href= ~
# í•´ë‹¹ ë…¼ë¬¸ í˜ì´ì§€ì— ë“¤ì–´ì™€ì„œëŠ” ë³´ì´ëŠ” Abstract class= "card-body acl-abstract"ì˜ span -> ì´ê±¸ ë¶„ì„
import requests
from bs4 import BeautifulSoup
import json
import re

# í¬ë¡¤ë§ ëŒ€ìƒ ì»¨í¼ëŸ°ìŠ¤ë“¤
conference_urls = {
    "ACL 2024": "https://aclanthology.org/volumes/2024.acl-long/",
    "EMNLP 2024": "https://aclanthology.org/volumes/2024.emnlp-main/",
    "NAACL 2024": "https://aclanthology.org/volumes/2024.naacl-long/"
}

def get_preview_sentences(text, num_sentences=2):
    sentences = re.split(r'(?<=[.!?])\s+', text)  # ë¬¸ì¥ ë¶„ë¦¬
    return " ".join(sentences[:num_sentences])

def fetch_paper_titles_and_links(url: str):
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, 'html.parser')
    paper_tags = soup.find_all("a", class_=lambda c: c and "align-middle" in c.split())

    papers = []
    for tag in paper_tags:
        title = tag.text.strip()
        # ì œëª©ì´ ë¹„ì–´ ìˆê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë©´ ê±´ë„ˆëœ€
        if not title or title.lower() in {"pdf", "bib", "abs"}:
            continue

        href = tag['href']
        link = href if href.startswith("http") else "https://aclanthology.org" + href

        papers.append({"title": title, "url": link})
    return papers

def fetch_abstract_and_authors(paper_url: str):
    response = requests.get(paper_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # Abstract
    abstract_div = soup.find("div", class_="card-body acl-abstract")
    abstract = abstract_div.find("span").text.strip() if abstract_div and abstract_div.find("span") else "Abstract not found"

    # Authors
    lead_p = soup.find("p", class_="lead")
    authors = lead_p.get_text(separator=", ").strip() if lead_p else "Authors not found"

    return abstract, authors

if __name__ == "__main__":
    all_results = []

    for conf_name, url in conference_urls.items():
        print(f"ğŸ” [{conf_name}] í¬ë¡¤ë§ ì¤‘...")
        papers = fetch_paper_titles_and_links(url)

        for i, paper in enumerate(papers, 1):
            try:
                # ì œëª©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€
                if not paper['title'].strip():
                    continue

                abstract, authors = fetch_abstract_and_authors(paper['url'])
                paper['abstract'] = abstract
                paper['authors'] = authors
                paper['conference'] = conf_name
                all_results.append(paper)

                preview = get_preview_sentences(abstract, num_sentences=2)

                print(f"âœ… ë…¼ë¬¸ {i} ({conf_name})")
                print(f"ì œëª©: {paper['title']}")
                print(f"ë§í¬: {paper['url']}")
                print(f"ì €ì: {paper['authors']}")
                print(f"Abstract (preview): {preview}\n{'-'*60}")

                # 20ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                if len(all_results) % 20 == 0:
                    with open("papers_partial_2024.json", "w", encoding="utf-8") as f:
                        json.dump(all_results, f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"âŒ ë…¼ë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n{'-'*60}")

    # ì „ì²´ ìµœì¢… ì €ì¥
    with open("papers_combined_2024.json", "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)