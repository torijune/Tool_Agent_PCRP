from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
RELEVANCE_PROMPT = """
ë‹¹ì‹ ì€ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ í‰ê°€ìì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ê²€ìƒ‰ëœ ë…¼ë¬¸ ìš”ì•½]
{retrieved_doc}

ì´ ë…¼ë¬¸ë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— 'ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆë‹¤ë©´' "accept", 'ì—°ê´€ì„±ì´ ë¶€ì¡±í•˜ê±°ë‚˜ í‹€ë¦° ë°©í–¥ì´ë¼ë©´' "reject"ë¼ê³  ë‹¨ë…ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
"""

def relevance_check_node(state: dict) -> dict:
    query = state["query"]
    retrieved_doc = state.get("retrieved_doc", "")

    print("ğŸ§  Relevance Check ì‹œì‘")
    print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸:\n{query}")
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë…¼ë¬¸ ìš”ì•½ (ë¯¸ë¦¬ë³´ê¸°):\n{retrieved_doc[:300]}...")
    
    # LLMì—ê²Œ íŒë‹¨ ìš”ì²­
    response = llm.invoke(RELEVANCE_PROMPT.format(query=query, retrieved_doc=retrieved_doc))
    decision = response.content.strip().lower()

    print(f"âœ… íŒë‹¨ ê²°ê³¼: {decision}")
    print("-" * 60)

    # ê²°ê³¼ ë°˜í™˜
    return {**state, "relevance_decision": decision}

relevance_check_node = RunnableLambda(relevance_check_node)