from langchain_core.runnables import RunnableLambda
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.schema.document import Document
from typing import List
from dotenv import load_dotenv
import numpy as np
import json, os

load_dotenv()
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

DATA_PATH = "agents/abstract_agents/data/EMNLP_ACL_NAACL_Abstracts.json"
FAISS_PATH = "agents/abstract_agents/data/faiss_index"

def load_documents(path: str) -> List[Document]:
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return [
        Document(
            page_content=item["abstract"],
            metadata={"title": item.get("title", ""), "url": item.get("url", "")}
        )
        for item in data if item.get("abstract") and item.get("title")
    ]

if os.path.exists(FAISS_PATH):
    vectorstore = FAISS.load_local(
        FAISS_PATH,
        embeddings=embedding_model,
        index_name="papers",
        allow_dangerous_deserialization=True
    )
else:
    documents = load_documents(DATA_PATH)
    vectorstore = FAISS.from_documents(documents, embedding_model)
    vectorstore.save_local(FAISS_PATH, index_name="papers")

# ğŸ”¹ ìˆ˜ì •ëœ retriever_node
def retriever_node(state: dict) -> dict:
    query = state["query"]
    plan_desc = state.get("plan_desc", "")
    reject_number = state.get("reject_number", 0)
    print("Tool critic reject number: ", reject_number)

    # ì¿¼ë¦¬ ì„ë² ë”©
    query_emb = embedding_model.embed_query(query)
    plan_emb = embedding_model.embed_query(plan_desc)

    # FAISS ì¸ë±ìŠ¤ ë²¡í„°
    faiss_index = vectorstore.index
    stored_vectors = faiss_index.reconstruct_n(0, faiss_index.ntotal)

    def cosine_sim(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8)

    query_scores = [cosine_sim(query_emb, v) for v in stored_vectors]
    plan_scores = [cosine_sim(plan_emb, v) for v in stored_vectors]

    if reject_number == 0:
        combined_scores = [(i, (q + p) / 2) for i, (q, p) in enumerate(zip(query_scores, plan_scores))]
    elif reject_number == 1:
        combined_scores = [(i, (0.3 * q + 0.7 * p)) for i, (q, p) in enumerate(zip(query_scores, plan_scores))]
    elif reject_number == 2:
        combined_scores = [(i, (0.7 * q + 0.3 * p)) for i, (q, p) in enumerate(zip(query_scores, plan_scores))]
    else:
        # MMR ê²€ìƒ‰ (ë¬¸ì„œ ì œëª© í¬í•¨ëœ summaryë¡œ ëŒ€ì²´)
        docs = vectorstore.max_marginal_relevance_search(query, k=5, fetch_k=20)
        print("\n Top 5 ë¬¸ì„œë“¤ê³¼ Score ë¶„ì„ (MMR):")
        for doc in docs:
            print(f"- ì œëª©: {doc.metadata['title']}")
            print(f"  â¤· ìš”ì•½: {doc.page_content[:100]}...")
        combined = "\n\n".join([
            f"ì œëª©: {doc.metadata['title']}\nìš”ì•½: {doc.page_content}"
            for doc in docs
        ])
        return {**state, "retrieved_doc": combined}

    # ê³µí†µ ì²˜ë¦¬: Top-k ì •ë ¬
    top_k = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:5]
    print("\n Top 5 ë¬¸ì„œë“¤ê³¼ Score ë¶„ì„:")
    for i, score in top_k:
        doc = vectorstore.docstore._dict[vectorstore.index_to_docstore_id[i]]
        print(f"- ì œëª©: {doc.metadata['title']}")
        print(f"  â¤· Query Score: {query_scores[i]:.4f}")
        print(f"  â¤· Plan Score : {plan_scores[i]:.4f}")
        print(f"  â¤· í‰ê·  Score  : {score:.4f}")

    docs = [vectorstore.docstore._dict[vectorstore.index_to_docstore_id[i]] for i, _ in top_k]
    combined = "\n\n".join([
        f"ì œëª©: {doc.metadata['title']}\nìš”ì•½: {doc.page_content}"
        for doc in docs
    ])
    return {**state, "retrieved_doc": combined}

retriever_node = RunnableLambda(retriever_node)