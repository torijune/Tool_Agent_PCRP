import os
import openai

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableLambda

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)

ANSWER_PROMPT = """
ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ì•„ë˜ëŠ” ê´€ë ¨ëœ ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:

{retrieved_doc}

ìœ„ ë…¼ë¬¸ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
"""

def answer_gen_node(state: dict) -> dict:
    query = state["query"]
    retrieved_doc = state.get("retrieved_doc", "")

    print("ğŸ’¬ Answer Generation ì‹œì‘")
    print(f"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸:\n{query}")
    print(f"ğŸ“„ ê²€ìƒ‰ëœ ë…¼ë¬¸ ìš”ì•½ (ë¯¸ë¦¬ë³´ê¸°):\n{retrieved_doc[:300]}...")

    response = llm.invoke(ANSWER_PROMPT.format(query=query, retrieved_doc=retrieved_doc))
    generated_answer = response.content.strip()

    print(f"âœ… ìƒì„±ëœ ë‹µë³€:\n{generated_answer}")
    print("-" * 60)

    return {**state, "generated_answer": generated_answer}

answer_gen_node = RunnableLambda(answer_gen_node)